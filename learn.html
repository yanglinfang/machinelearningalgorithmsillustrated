<!-- Nav tabs -->
<ul class="nav nav-tabs" role="tablist">
	<li role="presentation" class="page"><a href="#home" aria-controls="home" role="tab" data-toggle="tab"><span class="glyphicon glyphicon-home" aria-hidden="true"></span></a></li>
	<li role="presentation" class="active page"><a href="#learn" aria-controls="learn" role="tab" data-toggle="tab">Learn</a></li>
	<li role="presentation" class="page"><a href="#play" aria-controls="play" role="tab" data-toggle="tab">Play</a></li>
</ul>

<!-- Tab panes -->
<div class="tab-content">
	<div role="tabpanel" class="tab-pane" id="learn">learn</div>
	<div role="tabpanel" class="tab-pane" id="play">play</div>
</div>

<div class="main">
	<!-- <p>The logistic loss is sometimes called cross-entropy loss. It's also known as log loss (In this case, the binary label is often denoted by {-1,+1}).</p> -->

	<div class="container-fluid">
		<div class="row">
			<div class="col-sm-12">
				<h3></h3>
			</div>
			<div class="col-sm-6">
				
				<div class="row">
					<div class="col-sm-6">
						<p><span class="larger">To demonstrate the mechanics of a simple feedforward neural network</span>, we'll use a toy example consisiting of a 2 dimentional feature vector <span class="mathy"> &lt;weight, height&gt; </span> to predict a binary outcome of healthy or unhealthy. You may notice, that a Neural Network without any hidden layers is just Logistic Regression.

						The point of the exercise is to train a model that produces outcomes as close to the true values as possible. Or in other words, to "minimize the loss".

						
						</p>
					</div>
					<div class="col-sm-6">
						<p>
						This is done in two main phases, namely, a <strong>feedforward</strong> phase where all the data passes forward through the network so that we can calculate the loss. In other words, on average, how off are we from the ground truth.

						In the second phase, we <strong>backpropagate</strong> through the network using gradient descent so that we can update our model parameters (weights).

						We repeat these two phases until the algorithm converges, or until our error is sufficiently small.
						</p>
					</div>
				</div>
					
				
			
				<!-- <p><span class="larger">Functions</span> </p> -->
				
				
			</div>
			<div class="col-sm-6">

				<input class="tab-state" id="tab_1" type="radio" name="functions" checked>
				<label for="tab_1" class="section-label">Forward Pass</label>
				<input class="tab-state" id="tab_2" type="radio" name="functions">
				<label for="tab_2" class="section-label">Back Propagation</label>
				<a class="pull-right mtop15">Switch to Matrix notation</a>

				 <section id="content_1" class="tab">
					
					<h4>Sum of Affine Transformations</h4>
					<p>
						$$s_j = x_{j1} = \sum_{k=1}^{k}w_{ik}x_{ik}$$ <br>
						<em>where \(k\) is the number of nodes in layer \(i\)
						</em>
					</p>
					<h4>Activation Function<br>
						<span class="subhead">Also known as "non-linearity", or "transformation function"</span>
					</h4>
					<p>
						$$\hat{y}  = \sigma(s_i) \ where  \ \sigma(z) = \frac{1}{1+e^{-z}}$$
					</p>

					<h4>Loss Function<br>
						<span class="subhead">Sometimes referred to as a "cost function" or "objective function"</span>
					</h4>
					<p>There are <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">different loss functions</a> to choose from. In neural networks, it is common to use Cross Entropy Loss (<em>where $p$ is the ground truth, and $q$ is the prediction</em>) (also referred to as Log-Likelihood, which is the multiclass version of Cross Entropy):  </p>

					<p>
						$$H(p,q) = -\sum p_i\,log\,q_i = -y\,log\,\hat{y} - (1-y)log(1-\hat{y})$$

						$$ J(\theta) = \frac{1}{N}\sum_{n=1}^{N}H(p_n,q_n) = - \frac{1}{N}\sum_{n=1}^{N} \left[y_n\,log\,\hat{y_n} - (1-y_n)log(1-\hat{y_n})\right]$$

						
					</p>
				</section>	
				<section id="content_2" class="tab">
					<h4>Compute Gradients</h4>
					<p>To perform gradient descent, we compute the derivatives of the log likelihood with respect to the output unitâ€™s weights (note that in the case of a single output unit (a Bernoulli distirbution) as in our example, the log likelihood is the cross entropy function). Using the chain rule (for a single examples), we get:</p>	

					$$ \begin{align}
					\frac{\partial J(\theta)}{\partial w_{ij}}  &=  \frac{\partial J(\theta)}{\partial y_j} \ \frac{\partial y_j}{\partial s_j}  \ \frac{\partial s_j}{\partial w_{ij}} \\	

					s_j &= \sum_{i=1} x_iw_{ij} 

					\end{align} $$


					Calculating each part on the right hand side, we have:

					$$ \begin{align}

					\frac{\partial J(\theta)}{\partial y_j} &=  \frac {-y_j} { \hat{y_j} } + \frac {1-y_j} { 1-\hat{y_j} } \\
															&=  \frac{ \hat{y_j} - y_j } { \hat{y_j} (1-\hat{y_j}) } \\

					\frac{\partial y_j}{\partial s_j} &= \hat{y_j}(1-\hat{y_j}) \\

					\frac{\partial s_j}{\partial w_{ij}} &= {x_i} 

					\end{align} $$

					where \(x_i\) is the activation of the ith node of the hidden layer. Combining things back together,

					$$ \begin{align}

					\frac{\partial J(\theta)}{\partial s_j} &= \hat{y_j} - y_j \\
					\frac{\partial J(\theta)}{\partial w_{ij}} &= (\hat{y_j} - y_j)\hat{y_j}

					\end{align} $$

					This gives us the gradient for the weights in the last layer of the network. We now need to calculate the error gradient for the weights of the lower layers.

					(see <a href="https://www.ics.uci.edu/~pjsadows/notes.pdf">https://www.ics.uci.edu/~pjsadows/notes.pdf</a>) for continuation.
				</section>
			</div>
		</div>

		<div class="row">
			<div class="col-sm-12 play-bar">
				<span class="back"><i class="material-icons">fast_rewind</i></span>
				<span class="back"><i class="material-icons">skip_previous</i></span>
				<span class="play"><i class="material-icons">play_arrow</i></span>
				<span class="back"><i class="material-icons">skip_next</i></span>
				<span class="forward"><i class="material-icons">fast_forward</i></span>
			</div>

		</div>	


		<div class="row">
			<div class="col-sm-3" id="ScatterLR">
				<div class="tooltip hidden">
			        <span class="x1"></span><br>
			        <span class="x2"></span><br>
			        <span class="y"></span>
				</div>
				<div class="section-label">Data</div>
				<!-- <img src="img/data-LR-chart.png"\> -->
			</div>
			<div class="col-sm-3">
				<div class="section-label">Model - Logistic Regression</div>
				<div class="sections-summary">
					Input  layer  :  i<br>
					Output  layer  :  j
				</div>
				<div id="LR" class="graph-container">
					<div class="tooltip node_tip hidden">
						<div class="layer"></div>
						<div class="nodeVal"></div>
					</div>
				</div>
			</div>
			<div class="col-sm-3">
				<div class="section-label">Loss</div>
				<img src="img/loss-placeholder.png"/>
				<div class="section-label">Gradient descent</div>
				<img src="img/gd-placeholder.png"/>
			</div>
			<div class="col-sm-3">
				<div class="section-label">Math</div>
			</div>
		</div>

		<div class="row">
			<div class="col-sm-12 play-bar">
				<span class="back"><i class="material-icons">fast_rewind</i></span>
				<span class="back"><i class="material-icons">skip_previous</i></span>
				<span class="play"><i class="material-icons">play_arrow</i></span>
				<span class="back"><i class="material-icons">skip_next</i></span>
				<span class="forward"><i class="material-icons">fast_forward</i></span>
			</div>
		</div>	

		<div class="row">
			<div class="col-sm-12 mbot20">
				<p>That was all great untill a couple of tall skinny guys came in caughing up hairballs, and our data was <span class="larger">no longer linearly separable</span>. Hidden layers to the rescue!</p>
			</div>
			<div class="col-sm-3" class="scatter-plot" id="ScatterNN">
				<div class="tooltip hidden">
			        <span class="x1"></span><br>
			        <span class="x2"></span><br>
			        <span class="y"></span>
				</div>
				<div class="section-label">Data</div>
			</div>
			<div class="col-sm-3">
				<div class="section-label">Model - Neural Network</div>
				<div class="sections-summary">
					Input  layer  :  i<br>
					Hidden layer : j<br>
					Output  layer  :  k
				</div>
				<div id="NN" class="graph-container">
					<div class="tooltip node_tip hidden">
						<div class="layer"></div>
						<div class="nodeVal"></div>
					</div>
				</div>
			</div>
			<div class="col-sm-3">
				<div class="section-label">Loss</div>
				<img src="img/loss-placeholder.png"/>
				<div class="section-label">Gradient descent</div>
				<img src="img/gd-placeholder.png"/>
			</div>
			<div class="col-sm-3">
				<div class="section-label">Math</div>
			</div>
		</div>
	</div>



</div>