<!-- Nav tabs -->
<ul class="nav nav-tabs" role="tablist">
	<li role="presentation" class="page"><a href="#home" aria-controls="home" role="tab" data-toggle="tab"><span class="glyphicon glyphicon-home" aria-hidden="true"></span></a></li>
	<li role="presentation" class="page"><a href="#learn" aria-controls="learn" role="tab" data-toggle="tab">Learn</a></li>
	<li role="presentation" class="active page"><a href="#play" aria-controls="play" role="tab" data-toggle="tab">Play</a></li>
	<li role="presentation" class="page"><a href="#glossary" aria-controls="glossary" role="tab" data-toggle="tab">Glossary</a></li>
	<li role="presentation" class="page"><a href="#math" aria-controls="math" role="tab" data-toggle="tab">Math</a></li>
</ul>

<!-- Tab panes -->
<div class="tab-content">
	<div role="tabpanel" class="tab-pane" id="learn">learn</div>
	<div role="tabpanel" class="tab-pane" id="play">play</div>
	<div role="tabpanel" class="tab-pane" id="glossary">glossary</div>
	<div role="tabpanel" class="tab-pane" id="math">math</div>
</div>

<div class="main">
	<!-- <p>The logistic loss is sometimes called cross-entropy loss. It's also known as log loss (In this case, the binary label is often denoted by {-1,+1}).</p> -->

	<div class="container-fluid">
		<div class="row">
			<div class="col-md-6">
				<div class="row">
					<div class="col-sm-6">
						<p><span class="larger">To demonstrate the mechanics of a simple feedforward neural network</span>, we'll use a toy example
							consisiting of a 2 dimentional feature vector <span class="mathy"> &lt;weight, height&gt; </span> to classify a person as healthy or unhealthy. You may notice, that a Neural Network without any hidden layers is just Logistic
							Regression. The object is to train a <span class="larger mathy">model</span> that produces outcomes as close to the true values as possible. Or in other words, to minimize the <span class="larger mathy">loss</span>.
						</p>
					</div>
					<div class="col-sm-6">
						<p>
							This is done in two main phases, namely, a <em>feedforward</em> phase where all the data passes forward through
							the network so that we can calculate the loss. In other words, on average, how off are we from the ground truth. In
							the second phase, we <em>backpropagate</em> through the network using gradient descent so that we can update
							our <span class="larger mathy">model parameters (weights)</span>. We repeat these two phases until the algorithm converges, or until the loss is sufficiently small.
						</p>

						<p>If you are not at all familiar with Neural Networks, <a href="https://github.com/stephencwelch/Neural-Networks-Demystified"
								target="_blank">@stephencwelch</a> explains them really well in a short video series <span class="hidden-sm hidden-xs">on the right:</span>							<span class="visible-sm-inline visible-xs-inline">below:</span></p>
					</div>

				</div>

			</div>



			<div class="col-md-6">
				<div class="scalable scalable-16-9">
					<div class="scalable-content">
						<iframe src="https://www.youtube.com/embed/bxe2T-V8XRs"></iframe>
					</div>
				</div>
			</div>
		</div>

		<div class="row">
			<div class="col-sm-12 play-bar">
				<!-- <span class="back" id="backLR"><i class="material-icons">fast_rewind</i></span>
				<span class="back"><i class="material-icons">skip_previous</i></span> -->
				<span class="play" id="playLR"><i class="material-icons">play_circle_outline</i></span>
				<!-- <span class="back"><i class="material-icons">skip_next</i></span>
				<span class="forward"><i class="material-icons">fast_forward</i></span> -->
			</div>

		</div>
		<div class="row">
			<div class="col-sm-12 mbot20 big-text">
				<p>A logisitc regression model will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. If the decision surface is a hyperplane, or in the case of this 2 dimensional toy example, a line, then the classification problem is linear, and the classes are <a href"https://en.wikipedia.org/wiki/Decision_boundary">linearly separable</a>. <strong>Press play</strong> above to see this model learn a linear decision boundary.</p>
			</div>
		</div>
		

		<div class="row">
			<div class="col-md-6">
				<div class="row">
					<div class="col-sm-6">
						<div class="section-label">Model - Logistic Regression</div>
						<div class="sections-summary">
							Input layer : i<br> Output layer : j
						</div>
						<div id="LR" class="graph-container">
							<div class="tooltip node_tip hidden">
								<div class="layer"></div>
								<div class="nodeVal"></div>
							</div>
						</div>
					</div>
					<div class="col-sm-6">
						<!-- <div class="tooltip hidden">
							<span class="x1"></span><br>
							<span class="x2"></span><br>
							<span class="y"></span>
						</div> -->
						<div class="section-label">Data &amp; Decision boundary (hyperplane)</div>
						<div id="LRcontour"></div>
					</div>

					
				</div>
			</div>

			<div class="col-md-6">
				<div class="row">
					<div class="col-sm-6">
						<div class="section-label">Training Loss</div>
						<div id="LRLoss" class="graph-container"></div>

						
					</div>

					<div class="col-sm-6">
						<div class="section-label">Model parameters (weights)</div>
						<div id="LRweights" class="graph-container"></div>

					</div>

				</div>
			</div>
		</div>

		<div class="row">
			<div class="col-sm-12 play-bar">
				<!-- <span class="back" id="backNN"><i class="material-icons">fast_rewind</i></span>
				<span class="back"><i class="material-icons">skip_previous</i></span> -->
				<span class="play" id="playNN"><i class="material-icons">play_circle_outline</i></span>
				<!-- <span class="back"><i class="material-icons">skip_next</i></span>
				<span class="forward"><i class="material-icons">fast_forward</i></span> -->
			</div>
		</div>

		<div class="row">
			<div class="col-sm-12 mbot20 big-text">
				<p>That was all great untill a couple of tall skinny guys came in caughing up hairballs, and our data was <span class="larger">no longer linearly separable</span>. 
					If the network has no hidden layers, then it can only learn linear problems like the one above. If it has at least one hidden layer, then it can learn problems with convex <a href"https://en.wikipedia.org/wiki/Decision_boundary">decision boundaries</a>. With more layers, it can learn more complex problems. <strong>Press play</strong> above to see this simple neural network with one hidden layer learn a decision boundary to fit the new data.</p>
			</div>
		</div>
		<div class="row">
			<div class="col-md-6">
				<div class="row">
					<div class="col-sm-6">
						<div class="section-label">Model - Neural Network</div>
						<div class="sections-summary">
							Input layer : i<br> Hidden layer : j<br> Output layer : k
						</div>
						<div id="NN" class="graph-container">
							<div class="tooltip node_tip hidden">
								<div class="layer"></div>
								<div class="nodeVal"></div>
							</div>
						</div>
					</div>
					<div class="col-sm-6" class="scatter-plot">
						<!-- <div class="tooltip hidden">
							<span class="x1"></span><br>
							<span class="x2"></span><br>
							<span class="y"></span>
						</div> -->
						<div class="section-label">Data &amp; Decision boundary (hyperplane)</div>
						<div id="NNcontour"></div>
					</div>
					
				</div>
			</div>
			<div class="col-md-6">
				<div class="row">
					<div class="col-sm-6">
						<div class="section-label">Training Loss</div>
						<div id="NNLoss"></div>
						
					</div>
					<div class="col-sm-6">
						<div class="section-label">Model parameters (weights)</div>
						<div id="NNweights" class="graph-container"></div>

					</div>
				</div>
			</div>
		</div>

	</div>
</div>