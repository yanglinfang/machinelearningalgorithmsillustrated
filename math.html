<!-- Nav tabs -->
<ul class="nav nav-tabs" role="tablist">
	<li role="presentation" class="page"><a href="#home" aria-controls="home" role="tab" data-toggle="tab"><span class="glyphicon glyphicon-home" aria-hidden="true"></span></a></li>
	<li role="presentation" class="page"><a href="#learn" aria-controls="learn" role="tab" data-toggle="tab">Learn</a></li>
	<li role="presentation" class="page"><a href="#play" aria-controls="play" role="tab" data-toggle="tab">Play</a></li>
	<li role="presentation" class="page"><a href="#glossary" aria-controls="glossary" role="tab" data-toggle="tab">Glossary</a></li>
	<li role="presentation" class="page active"><a href="#math" aria-controls="math" role="tab" data-toggle="tab">Math</a></li>
</ul>

<!-- Tab panes -->
<div class="tab-content">
	<div role="tabpanel" class="tab-pane" id="learn">learn</div>
	<div role="tabpanel" class="tab-pane" id="play">play</div>
	<div role="tabpanel" class="tab-pane" id="glossary">glossary</div>
	<div role="tabpanel" class="tab-pane" id="math">math</div>
</div>math

<div>

<h2>MATH</h2>
	
	<hr>	
	<h3>Functions</h3>
		<h4>I. Forward Pass</h4>
		<hr>
		<h5>Sum of Affine Transformations</h5>
		<p>
			$$s_j = x_{j1} = \sum_{k=1}^{k}w_{ik}x_{ik} + b_i$$ <br>
			<em>where \(k\) is the number of nodes in layer \(i\)
			</em>
		</p>
		<h5>Activation Function<br>
			<span class="subhead">Also known as "non-linearity", or "transformation function"</span>
		</h5>
		<p>
			$$\hat{y}  = \sigma(s_i) \ where  \ \sigma(z) = \frac{1}{1+e^{-z}}$$
		</p>

		<h5>Loss Function<br>
			<span class="subhead">Sometimes referred to as a "cost function" or "objective function"</span>
		</h5>
		<p>There are <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">different loss functions</a> to choose from. In neural networks, it is common to use Cross Entropy Loss (<em>where $p$ is the ground truth, and $q$ is the prediction</em>) (also referred to as Log-Likelihood, which is the multiclass version of Cross Entropy):  </p>

		<p>
			$$H(p,q) = -\sum p_i\,log\,q_i = -y\,log\,\hat{y} - (1-y)log(1-\hat{y})$$

			$$ J(\theta) = \frac{1}{N}\sum_{n=1}^{N}H(p_n,q_n) = - \frac{1}{N}\sum_{n=1}^{N} \left[y_n\,log\,\hat{y_n} - (1-y_n)log(1-\hat{y_n})\right]$$

			
		</p>
		<h4>II. Back Propagation</h4>
		<hr>
		<h5>Compute Gradients</h5>
		<p>To perform gradient descent, we compute the derivatives of the log likelihood with respect to the output unitâ€™s weights (note that in the case of a single output unit (a Bernoulli distirbution) as in our example, the log likelihood is the cross entropy function). Using the chain rule (for a single examples), we get:</p>	

		$$ \begin{align}
		\frac{\partial J(\theta)}{\partial w_{ij}}  &=  \frac{\partial J(\theta)}{\partial y_j} \ \frac{\partial y_j}{\partial s_j}  \ \frac{\partial s_j}{\partial w_{ij}} \\	

		s_j &= \sum_{i=1} x_iw_{ij} 

		\end{align} $$


		Calculating each part on the right hand side, we have:

		$$ \begin{align}

		\frac{\partial J(\theta)}{\partial y_j} &=  \frac {-y_j} { \hat{y_j} } + \frac {1-y_j} { 1-\hat{y_j} } \\
												&=  \frac{ \hat{y_j} - y_j } { \hat{y_j} (1-\hat{y_j}) } \\

		\frac{\partial y_j}{\partial s_j} &= \hat{y_j}(1-\hat{y_j}) \\

		\frac{\partial s_j}{\partial w_{ij}} &= {x_i} 

		\end{align} $$

		where \(x_i\) is the activation of the ith node of the hidden layer. Combining things back together,

		$$ \begin{align}

		\frac{\partial J(\theta)}{\partial s_j} &= \hat{y_j} - y_j \\
		\frac{\partial J(\theta)}{\partial w_{ij}} &= (\hat{y_j} - y_j)\hat{y_j}

		\end{align} $$

		This gives us the gradient for the weights in the last layer of the network. We now need to calculate the error gradient for the weights of the lower layers.

		(see <a href="https://www.ics.uci.edu/~pjsadows/notes.pdf">https://www.ics.uci.edu/~pjsadows/notes.pdf</a>) for continuation.
	<h5>Update weights</h5>	
	<p>Lorem ipsum dolor sit amet</p>

	<h4>Rinse, Lather, Repeat</h4>
</div>
