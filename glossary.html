
<div class="main">

	<div class="row">
			<div class="col-md-12">
				<h2>Glossary</h2>
				<ul class="glossary">
					<li>Node: <span>each circular node represents an artificial neuron.</span></li>
					<li>Neoron: <span>artificial neurons are the constitutive units in an artificial neural network.</span></li>
					<li>Synapses: <span>in neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another.</span></li>
					<li>Layer: <span>layers are made up of a number of interconnected 'nodes' which contain an 'activation function'.</span></li>
					<li>Hidden layer: <span>any layer between our input and output layer is called a hidden layer. Recently, researchers have built networks with many many hidden layers. These are known as a deep belief networks, giving rise to the term deep learning.</span></li>
					<li>Parameters: <span>weights and biases of all layers.</span></li>
					<li>Weight: <span>synaptic weight refers to the strength or amplitude of a connection between two nodes.</span></li>
					<li>Bias: <span>a bias value allows you to shift the activation function to the left or right.</span></li>
					<li>Activation function: <span>the activation function of a node defines the output of that node given an input or set of inputs.</span></li>
					<li>Transformation function</li>
					<li>Sigmoid function: <span>one common type of activation function which is very often used.</span></li>
					<li>Objective function: <span>the objective function is what you directly try to minimize during training.</span></li>
					<li>Affine transformation: <span>linear transformation, for example lines that were parallel before the transformation are still parallel. Scaling, rotation, reflection etcetera. With regards to neural networks it's usually just the input matrix multiplied by the weight matrix.</span></li>
					<li>Forward propagation: <span>calculate output based on input and weights/biases and activation function.</span></li>
					<li>Back propagation: <span>find optimal weights and biases given difference between predicted output and actual output (also called loss).</span></li>
					<li>Cost function: <span>cost function is a measure of "how good" a neural network did with respect to it's given training sample and the expected output.</span></li>
					<li>Cross-entropy loss: <span>also known as Bernoulli negative log-likelihood and Binary Cross-Entropy</span></li>
					<li>Decision boundary: <span>in a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class.</span></li>
					<li>Gradient descent: <span>find a local minimum of a function by going 'downhill' along the slop of the function.</span></li>
					<li>Hyperplane: <span>decision boundary becomes more than just 2D line.</span></li>
					<li>Loss function: <span>same as cost function</span></li>
					<li>Minimization: <span>find optimal weights and biases to minimize loss.</span></li>
					<li>Linearly separable: <span>data can be separated by a line.</span></li>
					<li>Non-linearity: <span>intput and output do no have linear relationship.</span></li>
				</ul>
			</div>	
	</div>



</div>
