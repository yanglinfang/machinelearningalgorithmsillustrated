
<div class="main">

	<div class="row">
			<div class="col-md-12">
				<h2>Glossary</h2>
				<ul class="glossary">

					<li><strong>Activation function</strong>: the activation function of a node defines the output of that node given an input or set of inputs.</li>
					<li><strong>Affine transformation</strong>: linear transformation, for example lines that were parallel before the transformation are still parallel. Scaling, rotation, reflection etcetera. With regards to neural networks it's usually just the input matrix multiplied by the weight matrix.</li>

					<li><strong>Back propagation</strong>: find optimal weights and biases given difference between predicted output and actual output (also called loss).</li>

					<li><strong>Bias</strong>: a bias value allows you to shift the activation function to the left or right.</li>
					

					<li><strong>Cost function</strong>: cost function is a measure of "how good" a neural network did with respect to it's given training sample and the expected output.</li>
					<li><strong>Cross-entropy loss</strong>: also known as Bernoulli negative log-likelihood and Binary Cross-Entropy</li>
					<li><strong>Decision boundary</strong>: in a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class.</li>
					<li><strong>Forward propagation</strong>: calculate output based on input and weights/biases and activation function.</li>

					<li><strong>Gradient descent</strong>: find a local minimum of a function by going 'downhill' along the slop of the function.</li>
					<li><strong>Hidden layer</strong>: any layer between our input and output layer is called a hidden layer. Recently, researchers have built networks with many many hidden layers. These are known as a deep belief networks, giving rise to the term deep learning.</li>
					
					<li><strong>Hyperplane</strong>: decision boundary becomes more than just 2D line.</li>
					<li><strong>Layer</strong>: layers are made up of a number of interconnected 'nodes' which contain an 'activation function'.</li>

					<li><strong>Linearly separable</strong>: data can be separated by a line.</li>
					<li><strong>Loss function</strong>: same as cost function</li>
					<li><strong>Minimization</strong>: find optimal weights and biases to minimize loss.</li>

					<li><strong>Neuron</strong>: artificial neurons are the constitutive units in an artificial neural network.</li>

					<li><strong>Node</strong>: each circular node represents an artificial neuron.</li>
					<li><strong>Non-linearity</strong>: intput and output do no have linear relationship.</li>

					<li><strong>Objective function</strong>: the objective function is what you directly try to minimize during training.</li>
					<li><strong>Parameters</strong>: weights and biases of all layers.</li>
					<li><strong>Sigmoid function</strong>: one common type of activation function which is very often used.</li>

					<li><strong>Synapses</strong>: in neuroscience and computer science, synaptic weight refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another.</li>
					
					<li><strong>Transformation function</strong>: Same as activation function.</li>

					<li><strong>Weight</strong>: synaptic weight refers to the strength or amplitude of a connection between two nodes.</li>
					
				</ul>
			</div>	
	</div>



</div>
